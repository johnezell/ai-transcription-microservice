FROM pytorch/pytorch:2.1.2-cuda12.1-cudnn8-runtime

WORKDIR /app

# Install system dependencies
ENV DEBIAN_FRONTEND=noninteractive
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libsndfile1 \
    git \
    wget \
    curl \
    build-essential \
    tzdata \
    && ln -fs /usr/share/zoneinfo/UTC /etc/localtime \
    && dpkg-reconfigure --frontend noninteractive tzdata \
    && rm -rf /var/lib/apt/lists/*

# Install cuDNN 8 to match PyTorch expectations
RUN conda install -c conda-forge cudnn=8.9.2 --yes

# Create symbolic links for cuDNN 8 libraries to fix runtime loading
RUN cd /opt/conda/lib && \
    ln -sf libcudnn_ops_infer.so.8.9.2 libcudnn_ops_infer.so.8 && \
    ln -sf libcudnn.so.8.9.2 libcudnn.so.8 && \
    ln -sf libcudnn_adv_infer.so.8.9.2 libcudnn_adv_infer.so.8 && \
    ln -sf libcudnn_cnn_infer.so.8.9.2 libcudnn_cnn_infer.so.8

# Copy requirements first for better caching
COPY app/services/transcription/requirements.txt .

# Install dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Set environment variables for WhisperX
ENV PYTHONUNBUFFERED=1
ENV FLASK_APP=service.py
ENV FLASK_ENV=development
ENV CUDA_VISIBLE_DEVICES=0
ENV TORCH_HOME=/app/models
ENV HF_HOME=/app/models/huggingface
ENV WHISPERX_CACHE_DIR=/app/models/whisperx

# Create model cache directories
RUN mkdir -p /app/models/whisperx /app/models/huggingface

# Increase memory limits for WhisperX processing
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# Command to run the service
CMD ["python", "service.py"]