services:
  laravel:
    build:
      context: .
      dockerfile: Dockerfile.laravel
    container_name: laravel-app
    restart: unless-stopped
    ports:
      - "8080:80"
    volumes:
      - ./app/laravel:/var/www
      - ./app/shared:/var/www/storage/app/public/s3:delegated
      # Mount AWS credentials for profile-based authentication
      - ~/.aws:/mnt/aws_creds_mounted:ro
      # Mount D: drive for video downloads
      - D:/:/mnt/d_drive
    networks:
      - app-network
    environment:
      - APP_ENV=production
      - APP_DEBUG=false
      - DB_CONNECTION=sqlite
      - DB_DATABASE=/var/www/database/database.sqlite
      - CACHE_STORE=redis
      - SESSION_DRIVER=redis
      - QUEUE_CONNECTION=database
      - FILESYSTEM_DISK=d_drive
      - D_DRIVE_PATH=/mnt/d_drive
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=null
      - REDIS_DB=0
      - AUDIO_SERVICE_URL=http://audio-extraction-service:5000
      - TRANSCRIPTION_SERVICE_URL=http://transcription-service:5000
      # - MUSIC_TERM_SERVICE_URL=http://music-term-recognition-service:5000  # DISABLED - analytics service
      # AWS Configuration for credential resolution
      - AWS_PROFILE=truefire
      - AWS_SHARED_CREDENTIALS_FILE=/mnt/aws_creds_mounted/credentials
      - AWS_CONFIG_FILE=/mnt/aws_creds_mounted/config
      - AWS_DEFAULT_REGION=us-east-1
      - AWS_BUCKET=tfstream
      # Custom ngrok domain (persistent URL)
      - APP_URL=https://transcriptions.ngrok.dev
      # Force HTTPS for URL generation behind proxy
      - FORCE_HTTPS=true
      # Trust all proxies for Laravel 12
      - TRUSTED_PROXIES=*
      - TRUSTED_HEADERS=HEADER_X_FORWARDED_FOR,HEADER_X_FORWARDED_HOST,HEADER_X_FORWARDED_PORT,HEADER_X_FORWARDED_PROTO
      # Force Vite to use built assets instead of dev server
      - VITE_BUILD_ONLY=true
      - VITE_DEV_SERVER_ENABLED=false
      - VITE_HOT_RELOAD=false

  ngrok:
    image: ngrok/ngrok:latest
    container_name: ngrok-tunnel
    restart: unless-stopped
    command:
      - "start"
      - "--all"
      - "--config"
      - "/etc/ngrok.yml"
    volumes:
      - ./docker/ngrok/ngrok.yml:/etc/ngrok.yml
    ports:
      - "4040:4040"  # Ngrok web interface
    networks:
      - app-network
    environment:
      - NGROK_AUTHTOKEN=2yQOMwYL7PiKqJm9NfDtbnfirkb_6obd1p4jNHYCmzLuXKk8N
    depends_on:
      - laravel

  audio-extraction-service:
    build:
      context: .
      dockerfile: Dockerfile.audio-service
    container_name: audio-service
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./app/services/audio-extraction:/app
      - ./app/shared:/var/www/storage/app/public/s3:delegated
      - D:/:/mnt/d_drive
    networks:
      - app-network
    environment:
      - LARAVEL_API_URL=http://laravel/api
      - TRANSCRIPTION_SERVICE_URL=http://transcription-service:5000
      # Audio processing configuration - Force VAD to be completely disabled
      - ENABLE_VAD=false
      - ENABLE_NORMALIZATION=true
      - AUDIO_QUALITY_LEVEL=balanced
      - FFMPEG_THREADS=4
    ports:
      - "5050:5000"

  transcription-service:
    build:
      context: .
      dockerfile: Dockerfile.transcription
    container_name: transcription-service
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./app/services/transcription:/app
      - ./app/shared:/var/www/storage/app/public/s3:delegated
      - D:/:/mnt/d_drive
      # PERSISTENT MODEL CACHE - Prevents re-downloading models
      - whisperx_models:/app/models
      - huggingface_cache:/root/.cache/huggingface
    depends_on:
      - laravel
      - audio-extraction-service
      - ollama-service
    networks:
      - app-network
    environment:
      - LARAVEL_API_URL=http://laravel/api
      # LLM Configuration for guitar term evaluation - Now using containerized Ollama
      - LLM_ENDPOINT=http://ollama-service:11434/api/generate
      - LLM_MODEL=llama3.2:3b
      - LLM_ENABLED=true
      # MODEL CACHE CONFIGURATION - Prevents re-downloading
      - WHISPERX_CACHE_DIR=/app/models
      - HUGGINGFACE_HUB_CACHE=/root/.cache/huggingface
      - TORCH_HOME=/app/models/torch
    ports:
      - "5051:5000"

  # music-term-recognition-service: DISABLED - Reserved for future course analytics
  # This service provides categorized music term analysis and course insights.
  # Re-enable when ready to implement analytics dashboard and course analysis features.
  # 
  # music-term-recognition-service:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile.music-service
  #   container_name: music-service
  #   restart: unless-stopped
  #   volumes:
  #     - ./app/services/music-term-recognition:/app
  #     - ./app/shared:/var/www/storage/app/public/s3:delegated
  #   depends_on:
  #     - laravel
  #     - transcription-service
  #   networks:
  #     - app-network
  #   environment:
  #     - LARAVEL_API_URL=http://laravel/api
  #   ports:
  #     - "5052:5000"

  ollama-service:
    image: ollama/ollama:latest
    container_name: ollama-service
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ollama_data:/root/.ollama
      - ./docker/ollama/entrypoint.sh:/entrypoint.sh:ro
    networks:
      - app-network
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      # GPU configuration
      - CUDA_VISIBLE_DEVICES=0
      # Model configuration - specify which models to pull
      - OLLAMA_MODELS=llama3.2:3b,llama3.1:latest,mistral:7b-instruct,phi3:medium,phi3.5:latest,qwen2.5:7b
    ports:
      - "11435:11434"  # Changed to avoid conflict with local Ollama (11434)
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  redis:
    image: redis:7-alpine
    container_name: redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    networks:
      - app-network

volumes:
  redis_data:
  # PERSISTENT MODEL CACHES - Prevent re-downloading models on restart
  whisperx_models:
    driver: local
  huggingface_cache:
    driver: local
  # Ollama model storage - Prevent re-downloading models on restart
  ollama_data:
    driver: local

networks:
  app-network:
    driver: bridge
